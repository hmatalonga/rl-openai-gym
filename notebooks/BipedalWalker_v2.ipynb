{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "5mffsBmLC1U8",
    "outputId": "81863a56-4ead-49e6-b858-bcebc1c5b044"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.16.2\n",
      "0.12.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gym\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(0)\n",
    "\n",
    "print(np.__version__)\n",
    "print(gym.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e8i5ahkeVoAf"
   },
   "outputs": [],
   "source": [
    "def worker_process(arg):\n",
    "  get_reward_func, weights = arg\n",
    "  \n",
    "  return get_reward_func(weights)\n",
    "\n",
    "\n",
    "class EvolutionStrategy(object):\n",
    "  def __init__(self, weights,\n",
    "               get_reward_func,\n",
    "               population_size=20,\n",
    "               sigma=0.1,\n",
    "               learning_rate=0.01,\n",
    "               decay=0.999,\n",
    "               num_threads=1,\n",
    "               checkpoints=False):\n",
    "\n",
    "    self.weights = weights\n",
    "    self.get_reward = get_reward_func\n",
    "    \n",
    "    self.POPULATION_SIZE = population_size\n",
    "    self.SIGMA = sigma\n",
    "    \n",
    "    self.learning_rate = learning_rate\n",
    "    self.decay = decay\n",
    "    \n",
    "    self.num_threads = mp.cpu_count() if num_threads == -1 else num_threads\n",
    "    self.output_dir = 'output/'\n",
    "    self.checkpoints = checkpoints\n",
    "\n",
    "  def _get_weights_try(self, w, p):\n",
    "    weights_try = []\n",
    "    \n",
    "    for index, i in enumerate(p):\n",
    "      # w_try = w + sigma * N[j]\n",
    "      weights_try.append(w[index] + self.SIGMA * i)\n",
    "\n",
    "    return weights_try\n",
    "\n",
    "  \n",
    "  def get_weights(self):\n",
    "    return self.weights\n",
    "\n",
    "  \n",
    "  def _get_population(self):\n",
    "    population = []\n",
    "    \n",
    "    for _ in range(self.POPULATION_SIZE):\n",
    "      x = []\n",
    "      for w in self.weights:\n",
    "        x.append(np.random.randn(*w.shape))\n",
    "      population.append(x)\n",
    "    \n",
    "    return population\n",
    "  \n",
    "  \n",
    "  def _get_rewards(self, pool, population):\n",
    "    if pool is not None:\n",
    "      worker_args = ((self.get_reward, self._get_weights_try(self.weights, p)) for p in population)\n",
    "      rewards = pool.map(worker_process, worker_args)\n",
    "    else:\n",
    "      # R = np.zeros(npop)\n",
    "      rewards = np.zeros(self.POPULATION_SIZE)\n",
    "      for i, p in enumerate(population):\n",
    "        # w_try\n",
    "        weights_try = self._get_weights_try(self.weights, p)\n",
    "        # R[j] = f(w_try)\n",
    "        rewards[i] = self.get_reward(weights_try)\n",
    "\n",
    "    return np.array(rewards)\n",
    "\n",
    "\n",
    "  def _update_weights(self, rewards, population):\n",
    "    # np.std(R)\n",
    "    std = rewards.std()\n",
    "    \n",
    "    if std == 0:\n",
    "      return\n",
    "    \n",
    "    # A = (R - np.mean(R)) / np.std(R)\n",
    "    rewards = (rewards - rewards.mean()) / std\n",
    "    \n",
    "    for i, w in enumerate(self.weights):\n",
    "      # from list to array\n",
    "      layer_population = np.array([p[i] for p in population])\n",
    "      \n",
    "      # alpha/(npop*sigma)\n",
    "      update_factor = self.learning_rate / (self.POPULATION_SIZE * self.SIGMA)\n",
    "      \n",
    "      # np.dot(N.T, A)\n",
    "      pop_rewards = np.dot(layer_population.T, rewards).T\n",
    "      \n",
    "      # w = w + alpha/(npop*sigma) * np.dot(N.T, A)\n",
    "      self.weights[i] = w + update_factor * pop_rewards\n",
    "    \n",
    "    self.learning_rate *= self.decay\n",
    "\n",
    "  def run(self, iterations, print_step=10):\n",
    "    pool = mp.Pool(self.num_threads) if self.num_threads > 1 else None\n",
    "    results = []\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "      # N = np.random.randn(npop, 3)\n",
    "      population = self._get_population()\n",
    "      \n",
    "      # for j in range(npop):\n",
    "      rewards = self._get_rewards(pool, population)\n",
    "      \n",
    "      curr_reward = self.get_reward(self.weights)\n",
    "\n",
    "      self._update_weights(rewards, population)\n",
    "      results.append(curr_reward)\n",
    "\n",
    "      if (iteration + 1) % print_step == 0:\n",
    "        print('iter %d. reward: %f' % (iteration + 1, curr_reward))\n",
    "        \n",
    "      if self.checkpoints and (iteration + 1) % 50 == 0:\n",
    "        print('Saving checkpoint...')\n",
    "        \n",
    "        filename = 'bipedal-es_ep{}.p'.format((iteration + 1))\n",
    "        file_path = os.path.join(self.output_dir, filename)\n",
    "        pickle.dump(self.weights, open(file_path, 'wb'))\n",
    "        \n",
    "        filename = 'bipedal-es_results{}.p'.format((iteration + 1))\n",
    "        file_path = os.path.join(self.output_dir, filename)\n",
    "        pickle.dump(results, open(file_path, 'wb'))\n",
    "\n",
    "    if pool is not None:\n",
    "      pool.close()\n",
    "      pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yBREtX_0DN0x"
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "  \n",
    "  def __init__(self, obs_shape, act_shape, layer_shape=16, dtype=np.float64):\n",
    "    self.weights = [\n",
    "        np.zeros(shape=(obs_shape, layer_shape), dtype=dtype),\n",
    "        np.zeros(shape=(layer_shape, layer_shape), dtype=dtype),\n",
    "        np.zeros(shape=(layer_shape, act_shape), dtype=dtype)\n",
    "    ]\n",
    "\n",
    "\n",
    "  def predict(self, inp):\n",
    "    out = np.expand_dims(inp.flatten(), 0)\n",
    "    out = out / np.linalg.norm(out)\n",
    "    \n",
    "    for layer in self.weights:\n",
    "      out = np.dot(out, layer)\n",
    "    \n",
    "    return out[0]\n",
    "\n",
    "  def get_weights(self):\n",
    "    return self.weights\n",
    "\n",
    "  def set_weights(self, weights):\n",
    "    self.weights = weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "unGw7R5_Dabi"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "  AGENT_HISTORY_LENGTH = 1\n",
    "  POPULATION_SIZE = 32\n",
    "  EPS_AVG = 1\n",
    "  SIGMA = 0.1\n",
    "  LEARNING_RATE = 0.01\n",
    "  INITIAL_EXPLORATION = 1.0\n",
    "  FINAL_EXPLORATION = 0.0\n",
    "  EXPLORATION_DEC_STEPS = 1000000\n",
    "\n",
    "  def __init__(self):\n",
    "    self.env = gym.make('BipedalWalkerHardcore-v2')\n",
    "    \n",
    "    self.model = Model(24, 4, 78)\n",
    "    \n",
    "    self.es = EvolutionStrategy(\n",
    "        self.model.get_weights(),\n",
    "        self.get_reward,\n",
    "        self.POPULATION_SIZE,\n",
    "        self.SIGMA,\n",
    "        self.LEARNING_RATE,\n",
    "        checkpoints=True\n",
    "    )\n",
    "    \n",
    "    self.exploration = self.INITIAL_EXPLORATION\n",
    "    \n",
    "    self.output_dir = 'output/'\n",
    "    \n",
    "    \n",
    "  def summary(self):\n",
    "    print('Population size:', self.POPULATION_SIZE)\n",
    "    print('Learning rate:', self.LEARNING_RATE)\n",
    "    print('Sigma:', self.SIGMA)\n",
    "    print('Layers:', len(self.model.get_weights()))\n",
    "\n",
    "\n",
    "  def get_predicted_action(self, sequence):\n",
    "    prediction = self.model.predict(np.array(sequence))\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "\n",
    "  def load(self, filename='weights.p'):\n",
    "    file_path = os.path.join(self.output_dir, filename)\n",
    "    \n",
    "    self.model.set_weights(pickle.load(open(file_path,'rb')))\n",
    "    self.es.weights = self.model.get_weights()\n",
    "\n",
    "\n",
    "  def save(self, filename='weights.p'):\n",
    "    file_path = os.path.join(self.output_dir, filename)\n",
    "    \n",
    "    pickle.dump(self.es.get_weights(), open(file_path, 'wb'))\n",
    "\n",
    "\n",
    "  def play(self, episodes, render=True):\n",
    "    self.model.set_weights(self.es.weights)\n",
    "    # self.env = gym.wrappers.Monitor(self.env, 'video', force=True)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "      total_reward = 0\n",
    "      observation = self.env.reset()\n",
    "      sequence = [observation] * self.AGENT_HISTORY_LENGTH\n",
    "      done = False\n",
    "\n",
    "      while not done:\n",
    "        if render:\n",
    "          self.env.render()\n",
    "        action = self.get_predicted_action(sequence)\n",
    "        \n",
    "        observation, reward, done, _ = self.env.step(action)\n",
    "        observation = observation\n",
    "        \n",
    "        total_reward += reward\n",
    "        sequence = sequence[1:]\n",
    "        sequence.append(observation)\n",
    "\n",
    "      print(\"Total reward:\", total_reward)\n",
    "\n",
    "\n",
    "  def train(self, iterations):\n",
    "    self.es.run(iterations, print_step=1)\n",
    "\n",
    "\n",
    "  # f(w)\n",
    "  def get_reward(self, weights):\n",
    "    total_reward = 0.0\n",
    "    self.model.set_weights(weights)\n",
    "\n",
    "    for episode in range(self.EPS_AVG):\n",
    "      observation = self.env.reset()\n",
    "      sequence = [observation] * self.AGENT_HISTORY_LENGTH\n",
    "      done = False\n",
    "      \n",
    "      while not done:\n",
    "        self.exploration = max(\n",
    "            self.FINAL_EXPLORATION,\n",
    "            self.exploration - self.INITIAL_EXPLORATION / self.EXPLORATION_DEC_STEPS\n",
    "        )\n",
    "        \n",
    "        if random.random() < self.exploration:\n",
    "          action = self.env.action_space.sample()\n",
    "        else:\n",
    "          action = self.get_predicted_action(sequence)\n",
    "        \n",
    "        observation, reward, done, _ = self.env.step(action)\n",
    "        observation = observation\n",
    "        \n",
    "        total_reward += reward\n",
    "        sequence = sequence[1:]\n",
    "        sequence.append(observation)\n",
    "\n",
    "    return total_reward / self.EPS_AVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17833
    },
    "colab_type": "code",
    "id": "3m6bkER2EOZI",
    "outputId": "6b405c83-e546-4322-b420-755ec773cc78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: -91.28967385955082\n",
      "Total reward: -120.7362008030654\n",
      "Total reward: -151.93435643107284\n",
      "Total reward: -41.580282757850085\n",
      "Total reward: -116.03944843151905\n",
      "Total reward: -92.73282820205053\n",
      "Total reward: -145.8591758997851\n",
      "Total reward: -114.58805023258078\n",
      "Total reward: -111.15519262823149\n",
      "Total reward: -79.01692742220702\n"
     ]
    }
   ],
   "source": [
    "agent = Agent()\n",
    "agent.load('bipedal-es_ep350.p')\n",
    "agent.play(10)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ES2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
